{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CST4050 - Individual Assessment (block 1)\n",
    "\n",
    "**Student**:\n",
    "\n",
    "- Name: Thanima\n",
    "- Surname:Firoz\n",
    "- Student number:M00849665\n",
    "- Campus:Dubai\n",
    "- Group number:3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "Data processing in machine learning is a critical step required to increase the quality of data. This is done by preparing (cleaning and organizing) raw data into an understandable and readable format suitable for building and training machine learning models. The dataset provided is a COVID dataset in CSV format with 126 rows and 19476 columns. It has categorical attributes such as Sex, Severity, Age, and genome sequences that are continuous variables. There are no null values present in the dataset. In this case, Severity being our target/dependent variable and categorical in the nature of NonICU and ICU will be encoded into numeric values 0 and 1, respectively, as machine learning algorithms perform best with numeric values. The dataset also shows the continuous variable (genomes), which is our independent variable, which has values that vary in different ranges, which can affect the efficiency of our model. Normalization and Standardization will therefore be applied to enable the dataset to have a common scale and get a normal distribution of the dataset. By this, the mean of the dataset will be made 0, and the standard deviation equivalent to 1. The large nature of our predictors will require that we perform the Principal Component Analysis (PCA) to reduce the dimensionality of data while retaining as much information as possible to enhance the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         Libraries and Packages\n",
    "\n",
    "List of all packages used in the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                          Downloading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Dataset\n",
    "\n",
    "Reading the dataset from the given CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\matebook x\\Desktop\\Data Science\\ML\\cw\\covid_data.csv'\n",
    "genomic_df1 =pd.read_csv(filename)\n",
    "genomic_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Data Pre-Processing\n",
    "\n",
    "Converting the data into usable format. \n",
    "\n",
    "Following modifications has been done to the data to get most out of it:\n",
    "\n",
    "Removing Sample column as it is not contributing anything to the prediction.\n",
    "\n",
    "Checking null values if any.\n",
    "\n",
    "Applying Feature Engineering-Apply feature encoding and feature binning techniques \n",
    "\n",
    "Feature Encoding-Most of the ML algorithms cannot handle categorical variables and it is important to do feature encoding.\n",
    "\n",
    "Sex Column-Applying One Hot Encoding method inorder to convert from categorical to numerical variable and splits the category each to a column.It creates three different columns each for male,female and unknown and replaces one column with 1 and rest if the columns will be 0.\n",
    "\n",
    "Severity Column-Applying Label Encoding to transform from categorical into numerical variable by assigning a numerical value to each of categories.\n",
    "Binning the age column.\n",
    "\n",
    "Feature Binning-This technique is used to convert continuous variable to categorical.\n",
    "\n",
    "Age column-Applying Equal Frequency unsupervised feature binning to transform from numerical to categorical bins without considering the target cladd label into account and then applying one hot encoding technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the datatypes.\n",
    "print(genomic_df1.dtypes)\n",
    "\n",
    "# checking columns having string/object datatype.\n",
    "genomic_df1.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature  Binning the age column\n",
    "\n",
    "agerange=genomic_df1['Age']\n",
    "bins = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "labels = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79, 80-89','90-99']\n",
    "agerange1 = pd.cut(agerange, bins, labels = labels,include_lowest = True)\n",
    "genomic_df1.drop('Age',inplace=True,axis=1)\n",
    "df = pd.concat((genomic_df1, agerange1), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding Severity column.\n",
    "\n",
    "cols=['Severity']\n",
    "df[cols]=df[cols].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Deleting Sample column\n",
    "drop_columns=['Sample']\n",
    "df.drop(drop_columns,inplace=True,axis=1)\n",
    "\n",
    "#seperating the Severity lable column\n",
    "\n",
    "without_Severity_column = df.drop('Severity', axis = 1)       \n",
    "Severity_column = df['Severity']\n",
    "\n",
    "#Seperating columns for one hot coding.\n",
    "\n",
    "colums_to_convert = ['Age','Sex']   \n",
    "colums_to_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_Severity_column = pd.get_dummies(without_Severity_column, columns = colums_to_convert)    \n",
    "\n",
    "#performing one hot coding\n",
    "\n",
    "without_Severity_column.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the row with unknown sex value\n",
    "\n",
    "drop_columns=['Sex_unknown']\n",
    "without_Severity_column.drop(drop_columns,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Severity column again at the last position.\n",
    "\n",
    "final_data = pd.concat([without_Severity_column, Severity_column], axis = 1)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for any nullvalues in the dataframe.\n",
    "final_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                             Data Analysis\n",
    " \n",
    "Visualising the pre precessed data and trying to get the intution about different characterstics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICU_admission_distribution = final_data['Severity'].value_counts()\n",
    "print(\"Total Patients after pre processing: \", sum(ICU_admission_distribution))\n",
    "print(\"Distribution of ICU admissions\")\n",
    "print(\"Patients who were not admitted to ICU: \",ICU_admission_distribution[0])\n",
    "print(\"Patients who were admitted to ICU: \",ICU_admission_distribution[1])\n",
    "labels= ['Admitted to ICU', 'Not Admitted to ICU']\n",
    "colors=['tomato', 'deepskyblue']\n",
    "sizes= [ICU_admission_distribution[1], ICU_admission_distribution[0]]\n",
    "plt.pie(sizes,labels=labels, colors=colors, startangle=90, autopct='%1.1f%%')\n",
    "plt.title(\"ICU Distribution of data\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Age_distribution = final_data['Age_70-79, 80-89'].value_counts()\n",
    "print(\"Age Distribution\")\n",
    "print(\"Patients below age 70: \",Age_distribution[0])\n",
    "print(\"Patients above age 70: \",Age_distribution[1])\n",
    "labels= ['Below 70', 'Above 70']\n",
    "colors=['lightgreen', 'violet']\n",
    "sizes= [Age_distribution[0], Age_distribution[1]]\n",
    "plt.pie(sizes,labels=labels, colors=colors, startangle=90, autopct='%1.1f%%')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title(\"Age Distribution of data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = final_data['Sex_male'].value_counts()\n",
    "print(\"Total Patients after pre processing: \", sum(Gender))\n",
    "print(\"Gender of Patients\")\n",
    "print(\"Female Patients: \",Gender[0])\n",
    "print(\"Male Patients: \",Gender[1])\n",
    "labels= ['Male Patients', 'Female Patients']\n",
    "colors=['lightgreen', 'violet']\n",
    "sizes= [Gender[0], Gender[1]]\n",
    "plt.pie(sizes,labels=labels, colors=colors, startangle=90, autopct='%1.1f%%')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title(\"Gender of Patients\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    Dimensionality Reduction\n",
    "\n",
    "Dimensionality Reduction transforms the data from a high number of features into a lower number of features.In the given data,the  number of observations are less compared to  numbers of features.This can increase the variance in data,which could cause overfitting.Moreover the combinatorial explosion or a large number of values would lead to a computational intractable problem where the process takes too long to finish.Here I am using Principal Component Analysis(PCA) for dimensionality reduction.A significant benefit of PCR is that by using the principal components, if there is some degree of multicollinearity between the variables in the dataset, this procedure should be able to avoid this problem since performing PCA on the raw data produces linear combinations of the predictors that are uncorrelated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating the genomic feature columns for dimensionality reduction\n",
    "Severity=final_data.iloc[:, -1]\n",
    "df1=final_data.iloc[:, :-10]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 60\n",
    "# PCA\n",
    "print('\\nRunning PCA ...')\n",
    "pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n",
    "X_pca = pca.fit_transform(x)\n",
    "\n",
    "print('Explained variance: %.4f' % pca.explained_variance_ratio_.sum())\n",
    "\n",
    "print('Individual variance contributions:')\n",
    "for j in range(n_comp):\n",
    "    print(pca.explained_variance_ratio_[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(pca.n_components_) + 1, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running Principal Component analysis, it is found that the model genomic information can be captured with 60 features as they seem to cover the maximum variance. Hence, the 60 new features in an entirely new dimension are chosen as the final set of genomic features removing the other least discriminative ones. The chosen 60 features seem to have an explained variance of 85.6 % which can be clearly visualized in the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupComponents(x, y, classLabels, x_name='PC1', y_name='PC2'):   \n",
    "    classDict = {}\n",
    "    classes = np.unique(classLabels)\n",
    "    for label in classes:\n",
    "        idx = np.where(classLabels == label)\n",
    "        classDict[label] = (x[idx], y[idx])\n",
    "    for lab in classes:\n",
    "        x, y = classDict[lab]\n",
    "        plt.scatter(x, y, label=lab,alpha=0.4)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel(f'Projection on {x_name}', fontsize=12)\n",
    "    plt.ylabel(f'Projection on {y_name}', fontsize=12)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data points projection along PCs\n",
    "projected1 = np.matmul(x, pca.components_[0])\n",
    "projected2 = np.matmul(x, pca.components_[1])\n",
    "projected3 = np.matmul(x, pca.components_[2])\n",
    "projected4 = np.matmul(x, pca.components_[3])\n",
    "projected5 = np.matmul(x, pca.components_[4])\n",
    "\n",
    "plt.scatter(projected1, projected2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupComponents(projected1, projected3, Severity,x_name='PC1', y_name='PC3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_df = pd.DataFrame(pca.components_.T[:, :60], columns=[f'PC{x}' for x in range(1,60+1)], index=df1.columns)\n",
    "expl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining pca with age and sex column\n",
    "\n",
    "df_pca = pd.concat([pd.DataFrame(data = X_pca),\n",
    "                    final_data[['Age_20-29',\t'Age_30-39',\t'Age_40-49',\t'Age_50-59',\t'Age_60-69',\t\n",
    "                    'Age_70-79, 80-89',\t'Age_90-99',\t'Sex_female',\t'Sex_male',\t'Severity']]],\n",
    "                    axis = 1)\n",
    "\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into X & Y\n",
    "X = df_pca.iloc[:,df_pca.columns!='Severity']\n",
    "# Separating out the target\n",
    "Y= df_pca.iloc[:,df_pca.columns=='Severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given dataset contains 19476 columns, where 19472 columns are genome sequences corresponding to a sample of a person of a certain age and gender. These predictors, which include the genome sequences, are required to classify the Severity for that sample, whether it is a Non-ICU or an ICU case. Since this analysis involves classification of Severity, a supervised learning algorithm for Classification needs to be applied.  The data, including the Sex, Age, and genome sequences reduced to 60 components using PCA, will be used to train and test the model. 80% of this data will be the training data, and the rest 20% will be used as the unseen test data. This model can be validated using k-fold cross validation later to validate the accuracy of this model using different folds of the data. Hyperparameter tuning will be implemented to find the best parameters to train the model. The behavior of a machine learning model can be under control by tuning the best parameters. GridSearchCV is one of the most common hyperparameter tuning techniques to fine-tune the parameters, where we provide a list of parameters, and each combination of those provided values is created to train models and see which combination(grid) provides the most accuracy. The combination that provides the best result of all the combinations is chosen as the hyperparameters which we use to train our actual model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 Importing various Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of datasets\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                      Performing Logistic Regression,Linear SVM,RBF SVM,Decision Tree and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "names = [\"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\",\"Logistic Regression\"]\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=100),\n",
    "    SVC(gamma=2, C=10),\n",
    "    \n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegressionCV()\n",
    "    \n",
    "]\n",
    "scores = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    model =  clf.fit(X_train, Y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(Y_test, y_pred)\n",
    "    scores.append(score)\n",
    "    \n",
    "for i in range(len(scores)):\n",
    "    print(names[i] + \" : \" + str(scores[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Results as follows.\n",
    "\n",
    "Linear SVM : 0.8157894736842105\n",
    "\n",
    "RBF SVM : 0.5263157894736842\n",
    "\n",
    "Gaussian Process : 0.5263157894736842\n",
    "\n",
    "Decision Tree : 0.6842105263157895\n",
    "\n",
    "Random Forest : 0.6578947368421053\n",
    "\n",
    "Logistic Regression : 0.7368421052631579\n",
    "\n",
    "Among this,The highest accuracy is for Linear Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using training data\n",
    "Model_linear=svm.SVC(kernel='linear',C=1)\n",
    "Model_linear.fit(X_train,Y_train)\n",
    "#test the model using testing data\n",
    "y_pred_linear=Model_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing  the results as accuracy, recall, precision and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy_linear=Model_linear.score(X_test,Y_test)\n",
    "print(\"Accuracy SVM (Linear Kernel):\",metrics.accuracy_score(Y_test,y_pred_linear))\n",
    "print(\"Recall SVM (linear Kernel):\",recall_score(Y_test,y_pred_linear))\n",
    "print(\"Precision SVM (linear Kernel):\",precision_score(Y_test,y_pred_linear))\n",
    "print(\"confusion matrix SVM (linear Kernel):\\n\",confusion_matrix(Y_test,y_pred_linear))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results as follows\n",
    "\n",
    "Accuracy SVM (Linear Kernel): 0.8157894736842105\n",
    "\n",
    "Recall SVM (linear Kernel): 0.9444444444444444\n",
    "\n",
    "Precision SVM (linear Kernel): 0.7391304347826086\n",
    "\n",
    "confusion matrix SVM (linear Kernel):\n",
    "\n",
    " [[14  6]\n",
    "\n",
    " [ 1 17]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "               Tuning the parameters of Linear SVM model using Grigsearchcv()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the optimazer on the SVM model\n",
    "\n",
    "Model_SVM=svm.SVC()\n",
    "\n",
    "#define the parameters for the SVM that we want to optimise\n",
    "\n",
    "hyperparameter_space=[{'C':[0.1,1,10,100],'gamma':[1,0.1,0.01,0.001],'kernel':['rbf','sigmoid']},{'C':[0.1,1,10,100],'kernel':['linear']}]\n",
    "\n",
    "# create our optimiser using the set of parametrs\n",
    "\n",
    "optimizer=GridSearchCV(Model_SVM,param_grid=hyperparameter_space,scoring=\"accuracy\",cv=2,return_train_score=True)\n",
    "\n",
    "# train the model using the optimizer (train with tuning to find the best set of parameters)\n",
    "\n",
    "optimizer.fit(X_train,Y_train)\n",
    "\n",
    "# print the results\n",
    "\n",
    "print(\"Optimal hyperparameter combination:\",optimizer.best_params_)\n",
    "print()\n",
    "print(\"Mean cross-validated training accuracy score:\",optimizer.best_score_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning results of Linear SVM model are as follows\n",
    "\n",
    "Optimal hyperparameter combination: {'C': 0.1, 'kernel': 'linear'}\n",
    "\n",
    "Mean cross-validated training accuracy score: 0.8295454545454546\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation is the process of evaluating a trained model on test data set. This provides the generalization ability of a trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model with different activation functions and finding model with best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best parameters set to train the module\n",
    "\n",
    "optimizer.best_estimator_.fit(X_train,Y_train)\n",
    "\n",
    "# use the trained model to test\n",
    "y_pred=optimizer.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "# print the test results\n",
    "print(\"Test accuracy:\",metrics.accuracy_score(Y_test,y_pred))\n",
    "print(confusion_matrix(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally we can conclude that ourselected model(Linear SVM) has an accuracy of 81.58%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the model accuracy, there are several parameters need to be tuned. Three major parameters including:\n",
    "1. Kernels: The main function of the kernel is to take low dimensional input space and transform it into a higher-dimensional space. It is mostly useful in non-linear separation problem.\n",
    "2. C (Regularisation): C is the penalty parameter, which represents misclassication or error term. The misclassication or error term tells the SVM optimisation how much error is bearable. This is how you can control the trade-o between decision boundary and misclassication term.when C is high it will classify all the data points correctly, also there is a chance to overt.\n",
    "3. Gamma: It denes how far inuences the calculation of plausible line of separation when gamma is higher, nearby points will have high inuence; low gamma means far away points also be considered to get the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                          Confusion matrix\n",
    "                                          \n",
    "Confusion matrix is presented for test data with the highest scoring feature subset and optimal parameters, where the rows correspond to the actual performed activities, while the columns correspond to the predicated activity labels. We can clearly see the number of false positives and false negatives are very low. It is intrepreted that there seem to be very less number of misclasified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                              Visualizing Confusion Matrix using Heatmap\n",
    "\n",
    "Visualizing  the results of the model in the form of a confusion matrix using matplotlib and seaborn.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS:** \n",
    "* 14 patients were predicted that they **will** will be admitted to ICU ,the Prediction was CORRECT (True-Positive)\n",
    "* 17 patients were predicted that they **will NOT** be admitted to ICU, the Prediction was CORRECT (True-Negative)\n",
    "* 6 patients were predicted that they **will** will be admitted to ICU but the Prediction was WRONG (False-Positive)\n",
    "* 1 patients were predicted that they **will NOT**  be admitted to ICU but the Prediction was WRONG (False-Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ass(Y_test,y_pred):\n",
    "  tn, fp, fn, tp = confusion_matrix(Y_test, y_pred).ravel()\n",
    "  accuracy=(tp+tn)/(tp+fp+fn+tn)\n",
    "  specificity = tn/(tn+fp)\n",
    "  sensitivity=tp/(tp+fn)\n",
    "  print(\"Accuracy:\",accuracy*100)\n",
    "  print(\"Sensitivity:\",sensitivity*100)\n",
    "  print(\"Specificity:\",specificity*100)\n",
    "  print(\"recall: \", metrics.recall_score(Y_test, y_pred))\n",
    "  print(\"f1: \", metrics.f1_score(Y_test, y_pred))\n",
    "  print(\"ROC_AUC_Score:\",roc_auc_score(Y_test, y_pred)*100)\n",
    "ass(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Applying Kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X, Y, cv=10)\n",
    "scores\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was used to make prediction using X_train and the predictionwas stored in an object named y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=optimizer.best_estimator_.predict(X_test)\n",
    "y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred predicts the probability of class being a zero and One.\n",
    "One means patient will be admitted to ICU and zero means patient will not be admitted to ICU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_test.loc[:,:]\n",
    "test['Severity'] = Y_test\n",
    "test['pred'] = y_pred\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of the model is to predict the severity of covid pateints based on genomic features.\n",
    "Predictions deal with input observations to learn the unknown pattern by making predictions on unseen data. The data is split into 80: 20 ratio as training and testing data.The training data is used to train the model. Once the model is trained, it is be used to predict the Severity of the unseen test data. A confusion matrix heatmap was plotted to check the predictive model's performance, and accuracy score and precision was be checked. The accuracy score obtained is 81.58% and hence we can presume that the the model is set to fit the data accurately. The confusion matrix can show the number of observations correctly classified. This model can then successfully predict the Severity of parents admitted with COVID cases.The model is  safe to proceed due to the lower number of False Negatives.However, higher precision may actually be desirable. As we are not diagnosing illness, but determining if a person is likely to need an ICU bed,false Positives may lead to beds being occupied unnecessarily. If beds are extremely limited (which they are increasingly becoming), this would not ideal.\n",
    "\n",
    "Support Vector Machines (SVM) is one of machine learning algorithms using supervised learning models for pattern recognition and for classification and regression analysis. \n",
    "\n",
    "The advantages of using SVM model are as follows.\n",
    "\n",
    "1)In the classification task, SVM is more favored than the other methods because SVM provides a global solution for data classification.\n",
    "2)SVM model is able to find non-linear seperation classes.\n",
    "2)SVM model is very effective in high dimensional spaces.\n",
    "3)SVM Model is relatevely memory efficient as it uses a subset of training points in the decision function(called support vectors).\n",
    "4)SVM model utilizes a regularisation parameter and can tune bias-varaince tradeoff.\n",
    "5)SVM model is effective in cases where number of dimensions is greater than the number of samples.\n",
    "\n",
    "The limitations of SVM model are as follows.\n",
    "\n",
    "1)SVM model will not perform well when data has more noise as the target classes will be overlapping.\n",
    "2)If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "3)SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of an algorithm/model can be expressed using the Big O Notation, which defines an upper bound of an algorithm, it bounds a function only from 1) time complexity,deal with how long the algorithm is executed and  2) space complexity,deal with how much memory is used by its algorithm.\n",
    "\n",
    "The complexity of SVM model are as follows.\n",
    "\n",
    "Training Time Complexity=O(n²),n is the number of training samples.\n",
    "\n",
    "Note: if n is large, avoid using SVM.\n",
    "\n",
    "\n",
    "Run-time Complexity= O(k*d)\n",
    "\n",
    "K= number of Support Vectors,d=dimentionality of the data\n",
    "\n",
    "Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the libsvm-based implementation scales between O(n features X n2 samples) and O(n features X n3 samples)depending on how efficiently the libsvm cache is used in practice (dataset dependent). If the data is very sparse n features should be replaced by the average number of non-zero features in a sample vector.For the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features.\n",
    "\n",
    "It is always better to reduce the dimension of the data to decrease the computation complexities.\n",
    "\n",
    "SVM algorithms are not scale invariant and hence the data should be scaled.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9586f42597c7f0bc73eb2dfac87218613aba80d6b555519692afd61dfdddc624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
